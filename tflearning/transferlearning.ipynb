{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1003830,"sourceType":"datasetVersion","datasetId":550917}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import models, transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:20:10.778366Z","iopub.execute_input":"2025-01-26T09:20:10.778652Z","iopub.status.idle":"2025-01-26T09:20:10.782995Z","shell.execute_reply.started":"2025-01-26T09:20:10.778629Z","shell.execute_reply":"2025-01-26T09:20:10.782069Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nclass CustomImageFolder(datasets.ImageFolder):\n    def __init__(self, root, transform=None, loader=None):\n        super().__init__(root, transform=transform, loader=loader)\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n\n        if sample is None:\n            print(f\"Skipping corrupted image: {path}\")\n            return self.__getitem__((index + 1) % len(self.samples))\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample, target\n\ndef safe_loader(path):\n    try:\n        img = Image.open(path)\n        img.verify()  # Verify if the image is corrupted\n        img = Image.open(path)  # Reopen after verification to ensure it's loaded properly\n        return img.convert(\"RGB\")\n    except Exception as e:\n        print(f\"Skipping corrupted image: {path}\")\n        return None\n\ndataset = CustomImageFolder(root='/kaggle/input/microsoft-catsvsdogs-dataset/PetImages', transform=transform, loader=safe_loader)\n\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\nval_size = int(0.1 * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:35:55.805648Z","iopub.execute_input":"2025-01-26T09:35:55.806034Z","iopub.status.idle":"2025-01-26T09:36:03.401160Z","shell.execute_reply.started":"2025-01-26T09:35:55.806006Z","shell.execute_reply":"2025-01-26T09:36:03.400200Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"model = models.resnet18(pretrained=True)\n# model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:36:03.402313Z","iopub.execute_input":"2025-01-26T09:36:03.402600Z","iopub.status.idle":"2025-01-26T09:36:03.593991Z","shell.execute_reply.started":"2025-01-26T09:36:03.402573Z","shell.execute_reply":"2025-01-26T09:36:03.593211Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"num_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Linear(num_features, 2)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:36:03.595302Z","iopub.execute_input":"2025-01-26T09:36:03.595535Z","iopub.status.idle":"2025-01-26T09:36:03.599893Z","shell.execute_reply.started":"2025-01-26T09:36:03.595515Z","shell.execute_reply":"2025-01-26T09:36:03.599025Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:36:03.600720Z","iopub.execute_input":"2025-01-26T09:36:03.600952Z","iopub.status.idle":"2025-01-26T09:36:03.636245Z","shell.execute_reply.started":"2025-01-26T09:36:03.600932Z","shell.execute_reply":"2025-01-26T09:36:03.635550Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:36:03.637032Z","iopub.execute_input":"2025-01-26T09:36:03.637242Z","iopub.status.idle":"2025-01-26T09:36:03.641897Z","shell.execute_reply.started":"2025-01-26T09:36:03.637224Z","shell.execute_reply":"2025-01-26T09:36:03.641203Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    correct_preds = 0\n    total_samples = 0\n\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        _, preds = torch.max(outputs, 1)\n        correct_preds += torch.sum(preds == labels)\n        total_samples += labels.size(0)\n\n        running_loss += loss.item()\n\n        if (batch_idx + 1) % 50 == 0:\n            batch_accuracy = (correct_preds.double() / total_samples).item()\n            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], \"\n                  f\"Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.4f}\")\n\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_accuracy = correct_preds.double() / total_samples\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    val_correct_preds = 0\n    val_total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n\n            _, preds = torch.max(outputs, 1)\n            val_correct_preds += torch.sum(preds == labels)\n            val_total_samples += labels.size(0)\n\n    val_epoch_loss = val_loss / len(val_loader)\n    val_epoch_accuracy = val_correct_preds.double() / val_total_samples\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:36:03.642647Z","iopub.execute_input":"2025-01-26T09:36:03.642933Z","iopub.status.idle":"2025-01-26T09:59:02.352650Z","shell.execute_reply.started":"2025-01-26T09:36:03.642899Z","shell.execute_reply":"2025-01-26T09:59:02.351765Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Batch [50/563], Loss: 0.1046, Accuracy: 0.8819\nEpoch [1/10], Batch [100/563], Loss: 0.1603, Accuracy: 0.8978\nEpoch [1/10], Batch [150/563], Loss: 0.1233, Accuracy: 0.9102\nEpoch [1/10], Batch [200/563], Loss: 0.1237, Accuracy: 0.9142\nEpoch [1/10], Batch [250/563], Loss: 0.1446, Accuracy: 0.9170\nEpoch [1/10], Batch [300/563], Loss: 0.1484, Accuracy: 0.9206\nEpoch [1/10], Batch [350/563], Loss: 0.1396, Accuracy: 0.9248\nEpoch [1/10], Batch [400/563], Loss: 0.1400, Accuracy: 0.9270\nEpoch [1/10], Batch [450/563], Loss: 0.0576, Accuracy: 0.9287\nEpoch [1/10], Batch [500/563], Loss: 0.1100, Accuracy: 0.9300\nEpoch [1/10], Batch [550/563], Loss: 0.0875, Accuracy: 0.9306\nEpoch [1/10], Loss: 0.1724, Accuracy: 0.9308\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [1/10], Validation Loss: 0.1099, Validation Accuracy: 0.9555\nEpoch [2/10], Batch [50/563], Loss: 0.1084, Accuracy: 0.9537\nEpoch [2/10], Batch [100/563], Loss: 0.0729, Accuracy: 0.9506\nEpoch [2/10], Batch [150/563], Loss: 0.0837, Accuracy: 0.9556\nEpoch [2/10], Batch [200/563], Loss: 0.0446, Accuracy: 0.9542\nEpoch [2/10], Batch [250/563], Loss: 0.1768, Accuracy: 0.9539\nEpoch [2/10], Batch [300/563], Loss: 0.0570, Accuracy: 0.9544\nEpoch [2/10], Batch [350/563], Loss: 0.1689, Accuracy: 0.9536\nEpoch [2/10], Batch [400/563], Loss: 0.0380, Accuracy: 0.9537\nEpoch [2/10], Batch [450/563], Loss: 0.0267, Accuracy: 0.9536\nEpoch [2/10], Batch [500/563], Loss: 0.1452, Accuracy: 0.9541\nEpoch [2/10], Batch [550/563], Loss: 0.1434, Accuracy: 0.9542\nEpoch [2/10], Loss: 0.1159, Accuracy: 0.9544\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [2/10], Validation Loss: 0.1533, Validation Accuracy: 0.9310\nEpoch [3/10], Batch [50/563], Loss: 0.0134, Accuracy: 0.9650\nEpoch [3/10], Batch [100/563], Loss: 0.0304, Accuracy: 0.9641\nEpoch [3/10], Batch [150/563], Loss: 0.0468, Accuracy: 0.9652\nEpoch [3/10], Batch [200/563], Loss: 0.0469, Accuracy: 0.9644\nEpoch [3/10], Batch [250/563], Loss: 0.0447, Accuracy: 0.9635\nEpoch [3/10], Batch [300/563], Loss: 0.0989, Accuracy: 0.9644\nEpoch [3/10], Batch [350/563], Loss: 0.0290, Accuracy: 0.9645\nEpoch [3/10], Batch [400/563], Loss: 0.2455, Accuracy: 0.9632\nEpoch [3/10], Batch [450/563], Loss: 0.0623, Accuracy: 0.9626\nEpoch [3/10], Batch [500/563], Loss: 0.1362, Accuracy: 0.9629\nEpoch [3/10], Batch [550/563], Loss: 0.0154, Accuracy: 0.9643\nEpoch [3/10], Loss: 0.0910, Accuracy: 0.9645\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [3/10], Validation Loss: 0.1135, Validation Accuracy: 0.9550\nEpoch [4/10], Batch [50/563], Loss: 0.0711, Accuracy: 0.9744\nEpoch [4/10], Batch [100/563], Loss: 0.0195, Accuracy: 0.9728\nEpoch [4/10], Batch [150/563], Loss: 0.0474, Accuracy: 0.9704\nEpoch [4/10], Batch [200/563], Loss: 0.1065, Accuracy: 0.9711\nEpoch [4/10], Batch [250/563], Loss: 0.0238, Accuracy: 0.9716\nEpoch [4/10], Batch [300/563], Loss: 0.0208, Accuracy: 0.9723\nEpoch [4/10], Batch [350/563], Loss: 0.0438, Accuracy: 0.9724\nEpoch [4/10], Batch [400/563], Loss: 0.1164, Accuracy: 0.9702\nEpoch [4/10], Batch [450/563], Loss: 0.0274, Accuracy: 0.9692\nEpoch [4/10], Batch [500/563], Loss: 0.0264, Accuracy: 0.9693\nEpoch [4/10], Batch [550/563], Loss: 0.0899, Accuracy: 0.9692\nEpoch [4/10], Loss: 0.0775, Accuracy: 0.9694\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [4/10], Validation Loss: 0.1362, Validation Accuracy: 0.9505\nEpoch [5/10], Batch [50/563], Loss: 0.0171, Accuracy: 0.9781\nEpoch [5/10], Batch [100/563], Loss: 0.0117, Accuracy: 0.9778\nEpoch [5/10], Batch [150/563], Loss: 0.0817, Accuracy: 0.9792\nEpoch [5/10], Batch [200/563], Loss: 0.1027, Accuracy: 0.9775\nEpoch [5/10], Batch [250/563], Loss: 0.0759, Accuracy: 0.9761\nEpoch [5/10], Batch [300/563], Loss: 0.0296, Accuracy: 0.9770\nEpoch [5/10], Batch [350/563], Loss: 0.0708, Accuracy: 0.9765\nEpoch [5/10], Batch [400/563], Loss: 0.0293, Accuracy: 0.9755\nEpoch [5/10], Batch [450/563], Loss: 0.0784, Accuracy: 0.9739\nEpoch [5/10], Batch [500/563], Loss: 0.0278, Accuracy: 0.9734\nEpoch [5/10], Batch [550/563], Loss: 0.1388, Accuracy: 0.9736\nEpoch [5/10], Loss: 0.0678, Accuracy: 0.9731\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [5/10], Validation Loss: 0.0842, Validation Accuracy: 0.9660\nEpoch [6/10], Batch [50/563], Loss: 0.0210, Accuracy: 0.9831\nEpoch [6/10], Batch [100/563], Loss: 0.0218, Accuracy: 0.9822\nEpoch [6/10], Batch [150/563], Loss: 0.0885, Accuracy: 0.9846\nEpoch [6/10], Batch [200/563], Loss: 0.0130, Accuracy: 0.9814\nEpoch [6/10], Batch [250/563], Loss: 0.1230, Accuracy: 0.9818\nEpoch [6/10], Batch [300/563], Loss: 0.1318, Accuracy: 0.9802\nEpoch [6/10], Batch [350/563], Loss: 0.0332, Accuracy: 0.9800\nEpoch [6/10], Batch [400/563], Loss: 0.0681, Accuracy: 0.9797\nEpoch [6/10], Batch [450/563], Loss: 0.1176, Accuracy: 0.9794\nEpoch [6/10], Batch [500/563], Loss: 0.0397, Accuracy: 0.9791\nEpoch [6/10], Batch [550/563], Loss: 0.1275, Accuracy: 0.9795\nEpoch [6/10], Loss: 0.0568, Accuracy: 0.9793\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [6/10], Validation Loss: 0.1165, Validation Accuracy: 0.9480\nEpoch [7/10], Batch [50/563], Loss: 0.0229, Accuracy: 0.9800\nEpoch [7/10], Batch [100/563], Loss: 0.0047, Accuracy: 0.9831\nEpoch [7/10], Batch [150/563], Loss: 0.0399, Accuracy: 0.9825\nEpoch [7/10], Batch [200/563], Loss: 0.0133, Accuracy: 0.9836\nEpoch [7/10], Batch [250/563], Loss: 0.0245, Accuracy: 0.9833\nEpoch [7/10], Batch [300/563], Loss: 0.0158, Accuracy: 0.9830\nEpoch [7/10], Batch [350/563], Loss: 0.0346, Accuracy: 0.9830\nEpoch [7/10], Batch [400/563], Loss: 0.0199, Accuracy: 0.9823\nEpoch [7/10], Batch [450/563], Loss: 0.0401, Accuracy: 0.9821\nEpoch [7/10], Batch [500/563], Loss: 0.1396, Accuracy: 0.9817\nEpoch [7/10], Batch [550/563], Loss: 0.0440, Accuracy: 0.9809\nEpoch [7/10], Loss: 0.0487, Accuracy: 0.9810\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [7/10], Validation Loss: 0.0890, Validation Accuracy: 0.9675\nEpoch [8/10], Batch [50/563], Loss: 0.0214, Accuracy: 0.9838\nEpoch [8/10], Batch [100/563], Loss: 0.0087, Accuracy: 0.9853\nEpoch [8/10], Batch [150/563], Loss: 0.0030, Accuracy: 0.9838\nEpoch [8/10], Batch [200/563], Loss: 0.0060, Accuracy: 0.9847\nEpoch [8/10], Batch [250/563], Loss: 0.0049, Accuracy: 0.9854\nEpoch [8/10], Batch [300/563], Loss: 0.1028, Accuracy: 0.9846\nEpoch [8/10], Batch [350/563], Loss: 0.0132, Accuracy: 0.9838\nEpoch [8/10], Batch [400/563], Loss: 0.0098, Accuracy: 0.9837\nEpoch [8/10], Batch [450/563], Loss: 0.0380, Accuracy: 0.9831\nEpoch [8/10], Batch [500/563], Loss: 0.0886, Accuracy: 0.9826\nEpoch [8/10], Batch [550/563], Loss: 0.0393, Accuracy: 0.9825\nEpoch [8/10], Loss: 0.0469, Accuracy: 0.9824\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [8/10], Validation Loss: 0.1144, Validation Accuracy: 0.9630\nEpoch [9/10], Batch [50/563], Loss: 0.0098, Accuracy: 0.9838\nEpoch [9/10], Batch [100/563], Loss: 0.0068, Accuracy: 0.9844\nEpoch [9/10], Batch [150/563], Loss: 0.0202, Accuracy: 0.9860\nEpoch [9/10], Batch [200/563], Loss: 0.0883, Accuracy: 0.9878\nEpoch [9/10], Batch [250/563], Loss: 0.0475, Accuracy: 0.9875\nEpoch [9/10], Batch [300/563], Loss: 0.0288, Accuracy: 0.9875\nEpoch [9/10], Batch [350/563], Loss: 0.0210, Accuracy: 0.9869\nEpoch [9/10], Batch [400/563], Loss: 0.0164, Accuracy: 0.9870\nEpoch [9/10], Batch [450/563], Loss: 0.0050, Accuracy: 0.9862\nEpoch [9/10], Batch [500/563], Loss: 0.0047, Accuracy: 0.9861\nEpoch [9/10], Batch [550/563], Loss: 0.0689, Accuracy: 0.9863\nEpoch [9/10], Loss: 0.0389, Accuracy: 0.9860\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [9/10], Validation Loss: 0.1115, Validation Accuracy: 0.9645\nEpoch [10/10], Batch [50/563], Loss: 0.0014, Accuracy: 0.9919\nEpoch [10/10], Batch [100/563], Loss: 0.0187, Accuracy: 0.9903\nEpoch [10/10], Batch [150/563], Loss: 0.0141, Accuracy: 0.9921\nEpoch [10/10], Batch [200/563], Loss: 0.0158, Accuracy: 0.9917\nEpoch [10/10], Batch [250/563], Loss: 0.0030, Accuracy: 0.9904\nEpoch [10/10], Batch [300/563], Loss: 0.0630, Accuracy: 0.9880\nEpoch [10/10], Batch [350/563], Loss: 0.0051, Accuracy: 0.9885\nEpoch [10/10], Batch [400/563], Loss: 0.0048, Accuracy: 0.9890\nEpoch [10/10], Batch [450/563], Loss: 0.0019, Accuracy: 0.9895\nEpoch [10/10], Batch [500/563], Loss: 0.0963, Accuracy: 0.9891\nEpoch [10/10], Batch [550/563], Loss: 0.0494, Accuracy: 0.9891\nEpoch [10/10], Loss: 0.0320, Accuracy: 0.9891\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nSkipping corrupted image: /kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch [10/10], Validation Loss: 0.1813, Validation Accuracy: 0.9430\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"model.eval()\ncorrect_preds = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n\n        correct_preds += torch.sum(preds == labels)\n        total_samples += labels.size(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_acc = correct_preds.double() / total_samples\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:59:53.458246Z","iopub.execute_input":"2025-01-26T09:59:53.458538Z","iopub.status.idle":"2025-01-26T09:59:53.463596Z","shell.execute_reply.started":"2025-01-26T09:59:53.458517Z","shell.execute_reply":"2025-01-26T09:59:53.462742Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.9346\n","output_type":"stream"}],"execution_count":51}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks - Andrej Karpathy -> MultiGrad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Value:\n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    epsilon = 1e-9 \n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    return self * (other + epsilon)**-1\n",
    "  \n",
    "  def __rtruediv__(self, other):\n",
    "    return self / other\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "  \n",
    "  def __rsub__(self, other):\n",
    "    return self - other\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def sigmoid(self):\n",
    "    x = self.data\n",
    "    sig = 1 / (1 + math.exp(-x))\n",
    "\n",
    "    out = Value(sig, (self, ), 'sigmoid')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += sig * (1 - sig) * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def log(self):\n",
    "    epsilon = 1e-9\n",
    "    x = max(self.data, epsilon)\n",
    "    out = Value(math.log(x), (self, ), 'log')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad = (1 / x) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def backward(self):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x, activation=\"tanh\"):\n",
    "    # w * x + b\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    if activation == \"tanh\":\n",
    "      out = act.tanh()\n",
    "    else:\n",
    "      out = act.sigmoid()\n",
    "    return out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "  \n",
    "  def __call__(self, x, activation=\"tanh\"):\n",
    "    outs = [n(x, activation=activation) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "  \n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x, activation=\"tanh\"):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, activation=activation)\n",
    "    return x\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n Convolutional Neural Networks for Visual Recognition - Stanford (recommended by karpathy for a more intuitive understanding of backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "Challenges of Image Classification: \n",
    "- Viewpoint variation\n",
    "- Illumination \n",
    "- Scale Variation\n",
    "- Deformations\n",
    "- Occlusion\n",
    "- Clutter\n",
    "- Intraclass Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components: \n",
    "- Score Function\n",
    "    - Maps raw data to class scores\n",
    "- Loss Function\n",
    "    - Quantifies agreement between the predictions and ground truth.\n",
    "\n",
    "This is in-turn cast as an optimisation problem in which we minimize the loss function. This is basic NN theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score function\n",
    "\n",
    "![scorefunctions](assets/scorefunctions.png \"scorefunctions\")\n",
    "\n",
    "- i = 1...N (N samples)\n",
    "- y = 1...K (K distinct categories)\n",
    "- D = 3 (RGB) * L * W\n",
    "- Correct class is expected to have a score higher than incorrect classes. \n",
    "- The output matrix is [Kx1] for K labels\n",
    "- Once we are done training, all we need are the parameters and we no longer need the training data.\n",
    "\n",
    "##### Bias Trick\n",
    "- Instead of keeping the bias and weights individually, we can combine bias to the xi matrix as an additional dimension that holds 1.\n",
    "\n",
    "##### Preprocessing\n",
    "- Subtraction mean from all the pixels to center data (Bringing it to [-1,1] from [-127,127] ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "#### SVM Loss\n",
    "![lossfunction1](assets/lossfunctions.png \"lossfunction1\")\n",
    "\n",
    "- The loss function wants all correct class scores to be greater than incorrect class scores by \"delta\", or else we accumulate loss. This is also called hinge loss. There also exists squared hinge loss, which penalises quadratically. This sometimes works better, however, the SVM loss is known to be more standard.\n",
    "\n",
    "\n",
    "#### Regularisation\n",
    "\"\n",
    "![lossfunctionwithregulaisation](assets/lossfunctions2.png \"lossfunctionwithregulaisation\")\n",
    "\n",
    "One issue with the above loss function is that there can be multiple sets of weights which are optimal with respect to loss, however, we need something that \"generalises\" on the problem better otherwise we will find ourselves with a model that does not perform well on unseen data.\n",
    "\n",
    "To mitigate this, we add the regularisation penalty, which is basically the sum of squares of all the weights multiplied by lambda(to be determined through cross validation) to the existing loss.\n",
    "\n",
    "TLDR: This is an extra loss feature to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in range(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "  \"\"\"\n",
    "  fully-vectorized implementation :\n",
    "  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "  - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "  - W are weights (e.g. 10 x 3073)\n",
    "  \"\"\"\n",
    "  # evaluate loss over all examples in X without using any for loops\n",
    "  # left as exercise to reader in the assignment\n",
    "  delta  = 1.0\n",
    "  scores = W.dot(X)\n",
    "\n",
    "  margins = np.maximum(0, scores - scores[y, np.arange(X.shape(1))] + delta)\n",
    "  margins[y, np.arange(X.shape(1))] = 0\n",
    "  loss_all = np.sum(margins, axis = 0)\n",
    "\n",
    "  return loss_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Softmax Classifier\n",
    "- Softmax is a generalization of Logistic Regression classifier for multiple classes.\n",
    "- Cross entropy loss is used to compare probability distributions of the predicted and actual outputs.\n",
    "- What the softmax function does is it takes model outputs and squashes them into 0-1 probability values.\n",
    "- The goal of the softmax classifer is to maximize the probability of the correct class. Done by minimizing the negative log likelihood of the correct class. Hence the loss function is simply the negative log of the softmax function.\n",
    "- This is basically Maximum Likelihood Estimation (MLE).\n",
    "- Add regularisation to this to penalise large weights and prevent overfitting.\n",
    "- To prevent numeric stability, it is advised to offset the predictions by the max number of(if the predictions are too large), which simply makes the max prediction 0 so that when you calculate the softmax function, the results are correct but they improve numerical stability as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs Softmax\n",
    "\n",
    "![svmvssoftmax](assets/svmvssoftmax.png \"svmvssoftmax\")\n",
    "\n",
    "An intuitive difference between the Softmax and SVM is how the classifiers look at the predictions and accumulate loss. Softmax on one hand is never fully satisfied with the predictions it comes up with. The SVM however, is happy if the correct class has a score higher than all incorrect classes by the delta margin. This could be though of both as a bug and a feature depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gradient Descent\n",
    "We have talked about the loss functions for SVM and Softmax. Now comes the important part, Optimization.\n",
    "\n",
    "Find the right parameters W, that minimize the loss function.\n",
    "\n",
    "Its generally easier for gradient descent types of optimisation functions to do well on loss functions that have a convex structure. However. mostly this is not the case. Non linear functions are not convex, which makes reaching global minima challenging. \n",
    "\n",
    "The presence of \"kinks\" in the loss function makes the function non-differentiable at this point because there is no gradient defined. Subgradients are used int his case.\n",
    "\n",
    "Slope is defined as the instantaneous rate of change of the function at point. The gradient here at any point, points in the direction of \"steepest ascent\". The derivative quantifies how a small change in the parameter affects the loss function. How much does it affect the loss function. \n",
    "\n",
    "Considering, we have the gradients, we simply update the weights in the opposite direction with a given learning rate that slowly goes towards the steepest descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_Function(w):\n",
    "    return L(X, y, w)\n",
    "\n",
    "def evaluate_grad(f, x):\n",
    "    fx = f(x)\n",
    "    grad = np.zeros(x.shape)\n",
    "\n",
    "    h = 1e-4\n",
    "    x_copy = x.copy()\n",
    "    for i in range(len(x)):\n",
    "        x_copy[i] = x[i] + h\n",
    "\n",
    "        fxh = f(x_copy)\n",
    "\n",
    "        x_copy[i] = x[i]\n",
    "\n",
    "        grad[i] = (fxh - fx)/h\n",
    "\n",
    "    return grad\n",
    "\n",
    "def main():\n",
    "    W = ...\n",
    "    gradient = evaluate_grad(Loss_Function, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    learning_rate = 1e-6\n",
    "\n",
    "    W_new = W - learning_rate * gradient # direction of steepest descent\n",
    "    loss_new = Loss_function(W_new)\n",
    "\n",
    "    print(f\"for alpha {learning_rate}, the loss is : {loss_new}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "In the forward pass step, we calculate the output of the given neuron. Given W = .. and x = .., we calculate W[0] * x[0] + W[1] * x[1] + W[2] which gives us the prediction. Now we calculate the sigmoid of this (activation function that squashes the values between 0 & 1). Now that we have the final value, the next step is the backpropagate. Don't worry about the loss calculation or the weights updation right now.\n",
    "\n",
    "So what happens is we first calculate the gradient of the sigmoid function applied to the dot product, which gives us the initial gradient. Now we backpropagate and use this gradient to calculate the gradients for the individual variables that affect the predictions, which are the weights and inputs.\n",
    "\n",
    "ddot = (1 - f) * f -> standard derivative of sigmoid function.\n",
    "dx = [w[0] * ddot, w[1] * ddot] -> chain rule dL/dx[i] = dL/d(dot) * d(dot)/dx[i]\n",
    "above applies for dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3932238664829637, -0.5898357997244456]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n",
    "# we're done! we have the gradients on the inputs to the circuit\n",
    "\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.587432781397546"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "f(x,y) = x + sigma(y) / sigma(x) + (x+y)**2\n",
    "\"\"\"\n",
    "def sigma(x):\n",
    "    return 1.0 / 1 + math.exp(-x)\n",
    "\n",
    "# forward pass\n",
    "x = 3 \n",
    "y = -4\n",
    "\n",
    "sigx = sigma(x)\n",
    "sigy = sigma(y)\n",
    "\n",
    "num = x + sigy\n",
    "\n",
    "xpy = x + y\n",
    "xpysqr = xpy**2\n",
    "\n",
    "den = sigx + xpysqr\n",
    "\n",
    "invden = den**-1\n",
    "f = num * invden\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.5060758067319546, -1482.912912330901)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnum = invden\n",
    "dinvden = num\n",
    "\n",
    "dden = (-1 / den**2) * invden\n",
    "dsigx = 1 * dden\n",
    "dxpysqr = 1 * dden\n",
    "\n",
    "dxpy = (2 * xpy) * xpysqr\n",
    "dx = 1 * dxpy\n",
    "dy = 1 * dxpy \n",
    "\n",
    "dx += (1 - sigx) * sigx * dsigx\n",
    "dx += 1 * dnum\n",
    "dsigy = 1 * dnum\n",
    "\n",
    "dy += (1 - sigy) * sigy * dsigy\n",
    "\n",
    "dx, dy"
   ]
  },
  {
   "attachments": {
    "de01eb58-7434-4ec6-9036-c5fa8cb53803.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "- Activation functions take a single number and perform a fixed mathematical operation on it.\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "![sigmoid](https://cs231n.github.io/assets/nn1/tanh.jpeg \"sigmoid\")\n",
    "\n",
    "- Takes a real valued number and \"squashes\" it into range 0 and 1. In practice, it is fallen out of favor and is rarely ever used. 2 major drawbacks.\n",
    "\n",
    "    - Saturation: The activation saturatea at either tail of 0 or 1. The gradient at these regions is almost 0. This gradient will be multiplied to the gradient of this gate's output. If this is very small, it effectively kills the gradient and almost no signal will flow through the neuron.\n",
    "    - Not Zero Centered: Neurons later on would be receiving data is not zero centered. This has implications on the dynamics during gradient descent. If data coming in is always positive, then the gradient on the weights will be all positive or all negative.\n",
    "\n",
    "\n",
    "#### Tanh\n",
    "![tanh](https://cs231n.github.io/assets/nn1/tanh.jpeg \"tanh\")\n",
    "\n",
    "- Squashes real valued numbers in the range [-1,1]. Activations saturate here as well, but outputs are zero centered. Tanh non-linearity is always preferred over sigmoid.\n",
    "\n",
    "#### ReLU\n",
    "![relu](https://cs231n.github.io/assets/nn1/relu.jpeg \"relu\")\n",
    "\n",
    "The Rectified Linear Unit has become very popular in the last few years. \n",
    "\n",
    "- Known to accelerate the convergence of stochastic gradient descent, compared to other activation functions.\n",
    "- However, it is possible that ReLU units die during training. This could be because of a large gradient updating weights in a certain way, such that the neuron will never activate. This could cause a certain part of the neural network to simply be dead. With proper learning rate settings, this is rarely the case.\n",
    "\n",
    "#### Leaky ReLU\n",
    "\n",
    "![Screenshot 2024-12-27 at 23.14.01.png](attachment:de01eb58-7434-4ec6-9036-c5fa8cb53803.png)\n",
    "\n",
    "Where:\n",
    "- x is the input to the activation function.\n",
    "- α is a small constant (typically 0.01) which determines the slope for negative inputs.\n",
    "\n",
    "Some success has been seen with this function, but it is not known to be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Neurons in graphs.\n",
    "\n",
    "Modelled as collection of neurons that are connected in an acyclic graph. Cycles are not allowed to prevent forward pass loops. They are organised into distinct layers of neurons. The most common layer type is **fully connected layer** in which neurons between adjacent layers are fully pairwise connected, but neurons within a single layer share no connections. \n",
    "\n",
    "Types of layers: Input layer, hidden layers and the output layer.\n",
    "\n",
    "In a nueron consisting of an input layer with 3 neurons, a hidden layer with 4 neurons and an output layer with 2 neurons, there will be [3x4] + [4x2] = 20 weights and 4+2 = 6 biases, with a total 26 learnable parameters.\n",
    "\n",
    "\n",
    "#### The dual face of deep neural networks.\n",
    "![neural nets](https://cs231n.github.io/assets/nn1/layer_sizes.jpeg)\n",
    "\n",
    "The network with more neurons is able to express more complicated functions. However this is both a blessing and a curse. Simply because the 20 layer NN has obviously fit the noise in the data. This will not generalise well on the test set.\n",
    "\n",
    "This is not satisfying argument that smaller NNs are preferred. There are many proven ways to prevent overfitting, such as L2 regularization, dropout and input noise. In practice, this is preferable over smaller networks.\n",
    "\n",
    "Smaller networks are hard to train because they have few local minimas, but these minimas as easy to converge to, and they are bad. If you're trapped in one of such minima, your model is not going to work. On the other hand, if you train a large network, you'll find multiple solutions, but the variance in the loss will be much smaller. In other words, all solutions are about equally good.\n",
    "\n",
    "TLDR: Regularization strength is preferred to control the overfitting of a NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "#### Mean Subtraction\n",
    "- Most common form of processing.\n",
    "- Subtraction of meana across every individual feature in the data\n",
    "- ``` X -= np.mean(X, axis = 0)```\n",
    "\n",
    "#### Normalization\n",
    "- Refers to normalization of the data dimensions so that they are of approximately the same scale.\n",
    "- ```X /= np.std(X, axis = 0)```\n",
    "\n",
    "\n",
    "These steps are only necessary when you have reason to believe that different input features have different scales, but they should be of approximately equal importance to the learning algorithm. Not compulsory with images because they are already approximately equal and in the same range. \n",
    "\n",
    "#### Common pitfall\n",
    "The preprocessing statistic should always be computed on the training data, and the applied to the test/validation data.\n",
    "\n",
    "Computing the mean on the entire dataset and then splitting the data into train/val/test splits is a mistake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "When it comes to initializing weights, its reasonable to assume the approximately half of them would be negative and other half positive. However, setting all the initial weights to 0 would be a great mistake. This is because, if all the neurons produce the same output, then the updates would be identical throughout during backpropagation, which would prevent any source of asymmetry between neurons.\n",
    "\n",
    "#### The right way to do it\n",
    "Weights need to be close to zero, but not equal to zero. It is common to initialize them as small random numbers. This is referred to as **symmetry breaking**.\n",
    "\n",
    "```W = 0.01 * np.random.randn(D, H)```\n",
    "\n",
    "**But, why \"small\" random numbers you ask?**\n",
    "\n",
    "If the weights are too large, outputs can explode and the learning could become unstable. \n",
    "\n",
    "**Distribution**\n",
    "\n",
    "The distribution of outputs from the above weight vector neural network has a variance that grows with the number of inputs. We can normalize this variance to 1 by scaling the weight vector by the square root of the number of inputs.\n",
    "\n",
    "```W = np.random.randn(n) / sqrt(n)```\n",
    "\n",
    "This empirically improves the rate of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Initialization\n",
    "- Common to initialize biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights.\n",
    "- For ReLU non-linearities, however, some people like to use small constant values such as 0.01 for all biases because this ensures that all the units fire in the beginning and therefor propagate some gradient. Not clear if this provides a consistent improvement.\n",
    "\n",
    "\n",
    "**```W = np.random.randn(n) * sqrt(2/n)``` with ReLU units is the recommended practice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "#### L2 Regularization\n",
    "- Implemented by penalizing the squared magnitude of all parameters on the loss function. That is, for every weight w in the network, we add the term **(λw^2)/2**, where **λ** is the regularization strength. It is common to see the factor of 1/2 in front because then the gradient of this term with respect to the parameter **w** is simply **λw** instead of **2λw**.\n",
    "\n",
    "- Penalises peaky weight vectors and prefers diffuse ones.\n",
    "\n",
    "\n",
    "#### Dropout\n",
    "- Effective, simple and recently introduced.\n",
    "- It is implemented by keeping a neuron active with some proability **p** (a hyperparameter), or setting it to 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checks\n",
    "\n",
    "![gradientcorrect](assets/gradienttobeused.png)\n",
    "![gradientincorrect](assets/gradientdontuse.png)\n",
    "\n",
    "In practice, the centered one is preferred. We need to keep track of the relative error for comparison between the analytical and numerical gradients.\n",
    "\n",
    "|fa' - fn'| / max (|fa', fn'|, k)\n",
    "\n",
    "k is a small constant to prevent division by 0.\n",
    "This error is analysed as follows:\n",
    "\n",
    "- relative error > 1e-2 usually means the gradient is probably wrong\n",
    "- 1e-2 > relative error > 1e-4 should make you feel uncomfortable\n",
    "- 1e-4 > relative error is usually okay for objectives with kinks. But if there\n",
    "are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.\n",
    "- 1e-7 and less you should be happy.\n",
    "- Keep in mind that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of 1e-2 might be okay because the errors build up on the way\n",
    "- Use double precision.\n",
    "- Be careful about extremely small numbers in analytical grad / numerical grad (1e-10 or smaller). If they are, scale loss function by a constant otherwise, you will come across floating-point [issues](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)\n",
    "\n",
    "- Kinks, as discussed earlier could cause a numerical gradient to computer a zero gradient if reached. This should be handled by keeping track of max(x,y). That is, was x or y higher during the forward pass? If so, at least one winner changes when evaluating f(x+h) and then f(x-h), a kink was crossed.\n",
    "- Use fewer datapoints. This fixes the kink problem, since loss functions that contain kinks, will have fewer kinks with fewer datapoints. Kinks are introduced by using certain activation functions or loss functions.\n",
    "- Be careful with the step size h. It's not necessarily smaller the better, because when h is much smaller, you run into numerical precision problems. But sometimes, reducing also fixes the problem. Check this [chart](http://en.wikipedia.org/wiki/Numerical_differentiation) for more context.\n",
    "- Start performing gradient checks after a **burn-in** time because performing in the first iteration could introduce pathological edge cases and mask incorrect implementations. This is because an incorrect implementation could still produce almost the same pattern as a correct one, but will not generalize to a more characteristic mode of operation where some scores are larger than others.\n",
    "- Check the data loss and regularization loss individually. Because if this term brings in the majority loss, this could mask an incorrect implementation on the loss gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
